# Curso Completo de PySpark 4.0: Do B√°sico ao Avan√ßado

## üöÄ Vis√£o Geral do Curso

Bem-vindo ao Curso Completo de PySpark 4.0! Este material foi cuidadosamente elaborado para fornecer uma jornada de aprendizado profunda e pr√°tica no mundo do processamento de Big Data com Apache Spark.

Diferente de outros materiais, este curso √© focado na **vers√£o 4.0 do PySpark**, com exemplos e configura√ß√µes otimizadas para o **Google Colab**, e utiliza um conjunto de dados de ecommerce real (`clientes`, `produtos`, `vendas`) para criar laborat√≥rios pr√°ticos e contextualizados.
 
**Plataforma**: Google Colab

## üéØ Objetivos de Aprendizagem

Ao final deste curso, voc√™ ser√° capaz de:

*   Entender a arquitetura e os fundamentos do Apache Spark.
*   Configurar e otimizar um ambiente PySpark 4.0 no Google Colab.
*   Manipular, filtrar e transformar grandes volumes de dados com a API de DataFrames.
*   Realizar an√°lises complexas, cruzando dados de m√∫ltiplas fontes com `join` e `groupBy`.
*   Utilizar o poder do Spark SQL para executar consultas SQL em Big Data.
*   Construir, treinar e avaliar modelos de Machine Learning em escala com a biblioteca MLlib.
*   Processar dados em tempo real com o Structured Streaming.
*   Diagnosticar gargalos e otimizar a performance de suas aplica√ß√µes Spark usando a Spark UI.


## üõ†Ô∏è Como Come√ßar

1.  **Acesse o Google Colab**: Abra [Google Colab](https://colab.research.google.com/drive/1QlQiUuRNcbWFW3xLuY1BoUWlrPyZGxL8?usp=sharing).
2.  **Fa√ßa o Upload dos Datasets**: Fa√ßa o upload dos arquivos `clientes.csv`, `produtos.csv`, e `vendas.csv` (localizados no github) para o seu ambiente Colab.
3.  **Siga os M√≥dulos**: Prossiga pelos m√≥dulos na ordem, lendo a teoria e executando os laborat√≥rios pr√°ticos no seu notebook.

## üí° Diferenciais Deste Curso

*   **Foco no PySpark 4.0**: Utiliza a vers√£o mais recente do Spark, com suas novas funcionalidades e otimiza√ß√µes.
*   **Exemplos Pr√°ticos e Reais**: Todas as an√°lises s√£o baseadas em um cen√°rio de ecommerce real, respondendo a perguntas de neg√≥cio concretas.
*   **Explica√ß√µes Detalhadas**: Foco em explicar o **"porqu√™"** e o **"como"** por tr√°s de cada conceito, como o funcionamento interno de um `join` ou de um `groupBy`.
*   **M√£o na Massa**: O curso √© centrado em laborat√≥rios pr√°ticos que constroem conhecimento de forma incremental.
*   **Otimiza√ß√£o desde o In√≠cio**: Introduz conceitos de performance e o uso da Spark UI para que voc√™ aprenda a escrever c√≥digo eficiente.

--- 

**Pronto para dominar o processamento de Big Data? Vamos come√ßar!**
# M√≥dulo 0: Configurando o Ambiente com PySpark 4.0 no Google Colab

## 1. Introdu√ß√£o

Bem-vindo ao curso de PySpark! O primeiro passo em nossa jornada √© configurar um ambiente de desenvolvimento robusto e eficiente. Para isso, utilizaremos o **Google Colaboratory (Colab)**, uma plataforma gratuita que oferece acesso a recursos computacionais na nuvem, eliminando a necessidade de instala√ß√µes complexas em sua m√°quina local.

Neste m√≥dulo, vamos instalar e configurar o **PySpark 4.0**, a vers√£o mais recente do Spark, que traz melhorias significativas de performance e usabilidade, especialmente em ambientes como o Colab.

## 2. Por que PySpark 4.0 e Java 17?

- **PySpark 4.0**: Traz otimiza√ß√µes no Catalyst Optimizer, melhorias na API do Pandas e uma integra√ß√£o mais fluida com o ecossistema Python moderno.
- **Java 17 (JDK 17)**: O Spark 4.0 √© constru√≠do sobre o Scala 2.13, que por sua vez, funciona melhor com vers√µes mais recentes do Java. O uso do JDK 17 garante compatibilidade e acesso √†s √∫ltimas melhorias de performance e seguran√ßa da JVM.
- **`pyspark[connect]`**: Instalaremos o PySpark com o extra `[connect]`, que habilita o **Spark Connect**. O Spark Connect introduz uma arquitetura desacoplada cliente-servidor, permitindo que voc√™ execute suas aplica√ß√µes Spark de qualquer lugar (seu notebook, um IDE local) e se conecte a um cluster Spark remoto. Isso melhora a interatividade e a experi√™ncia de desenvolvimento.

## 3. Script de Instala√ß√£o para o Google Colab

Para come√ßar, crie um novo notebook no Google Colab. Na primeira c√©lula, cole e execute o seguinte c√≥digo. Este script cuidar√° de toda a instala√ß√£o e configura√ß√£o necess√°rias.

```python
# 1. Atualizar os pacotes do sistema operacional
!apt-get -qq update

# 2. Instalar o Java Development Kit (JDK) 17
# O PySpark 4.0 funciona melhor com vers√µes mais recentes do Java.
!apt-get -qq install -y openjdk-17-jdk-headless

# 3. Instalar o PySpark 4.0 com suporte ao Spark Connect
# O -U garante que estamos instalando ou atualizando para a vers√£o especificada.
!pip -q install -U pyspark[connect]==4.0.0

# 4. Mensagem de sucesso
print('‚úÖ PySpark 4.0 instalado com sucesso!')

# 5. AVISO IMPORTANTE:
# Ap√≥s a execu√ß√£o desta c√©lula, voc√™ DEVE reiniciar o ambiente de execu√ß√£o.
# V√° em "Ambiente de execu√ß√£o" > "Reiniciar ambiente de execu√ß√£o" no menu do Colab.
# Isso √© crucial para que as novas bibliotecas e vari√°veis de ambiente sejam carregadas corretamente.
```

### **Aten√ß√£o: Reinicie o Ambiente de Execu√ß√£o!**

Ap√≥s a instala√ß√£o ser conclu√≠da, √© **obrigat√≥rio** reiniciar o ambiente de execu√ß√£o do Colab. Voc√™ pode fazer isso clicando em **"Ambiente de execu√ß√£o"** na barra de menu e depois em **"Reiniciar ambiente de execu√ß√£o"**. Se voc√™ n√£o fizer isso, o notebook n√£o encontrar√° a nova instala√ß√£o do PySpark e os pr√≥ximos passos falhar√£o.

## 4. Iniciando a `SparkSession`

Ap√≥s reiniciar o ambiente, voc√™ pode iniciar sua `SparkSession`. A `SparkSession` √© o ponto de entrada para qualquer aplica√ß√£o Spark. Em uma nova c√©lula, execute o seguinte c√≥digo:

```python
# Importar a SparkSession do PySpark
from pyspark.sql import SparkSession

# Construir e criar a SparkSession
# .master("local[*]") -> Executa o Spark localmente usando todos os cores de CPU dispon√≠veis
# .appName("CursoHadoop") -> Define um nome para sua aplica√ß√£o
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("CursoHadoop") \
    .getOrCreate()

# Verificar se a sess√£o foi criada com sucesso
print("SparkSession criada com sucesso!")
print(spark)

# Exibir a vers√£o do Spark
print(f"Vers√£o do Spark: {spark.version}")
```

Se tudo correu bem, voc√™ ver√° uma mensagem de sucesso e os detalhes da sua `SparkSession`, confirmando que voc√™ est√° executando a vers√£o 4.0.0.

## 5. Carregando os Dados do Curso

Para os laborat√≥rios deste curso, utilizaremos tr√™s arquivos CSV que voc√™ forneceu: `clientes.csv`, `produtos.csv` e `vendas.csv`.

Primeiro, fa√ßa o upload desses arquivos para o seu ambiente Colab. Voc√™ pode fazer isso clicando no √≠cone de pasta na barra lateral esquerda e, em seguida, no bot√£o "Fazer upload".

Depois de fazer o upload, voc√™ pode carregar os dados em DataFrames do PySpark:

```python
# Carregar os datasets em DataFrames
# header=True -> Usa a primeira linha como cabe√ßalho
# inferSchema=True -> Tenta inferir os tipos de dados das colunas (pode ser lento para arquivos grandes)

clientes_df = spark.read.csv('clientes.csv', header=True, inferSchema=True)
produtos_df = spark.read.csv('produtos.csv', header=True, inferSchema=True)
vendas_df = spark.read.csv('vendas.csv', header=True, inferSchema=True)

# Exibir as primeiras linhas e o schema de cada DataFrame para verificar
print("--- Clientes ---")
clientes_df.show(5)
clientes_df.printSchema()

print("--- Produtos ---")
produtos_df.show(5)
produtos_df.printSchema()

print("--- Vendas ---")
vendas_df.show(5)
vendas_df.printSchema()
```

Com o ambiente configurado e os dados carregados, voc√™ est√° pronto para come√ßar a explorar o poder do PySpark nos pr√≥ximos m√≥dulos!
# M√≥dulo 1: Fundamentos do Apache Spark

## Cap√≠tulo 1: O que √© Apache Spark?

### 1.1. Definindo o Apache Spark

O **Apache Spark** √© uma plataforma de computa√ß√£o distribu√≠da de c√≥digo aberto, unificada e de alta velocidade, projetada para o processamento de dados em larga escala (Big Data). Ele se destaca por sua capacidade de executar tarefas de an√°lise de dados de forma significativamente mais r√°pida do que tecnologias antecessoras, como o Hadoop MapReduce, principalmente por realizar o processamento em mem√≥ria.

Em sua ess√™ncia, o Spark √© um motor de processamento. Ele n√£o armazena dados de forma nativa e permanente, mas se integra a uma vasta gama de sistemas de armazenamento, como Hadoop Distributed File System (HDFS), Amazon S3, Google Cloud Storage, e bancos de dados NoSQL como Cassandra e HBase. Essa flexibilidade permite que o Spark atue como uma camada de processamento poderosa sobre os dados, onde quer que eles residam.

> "O Spark foi projetado para cobrir uma ampla gama de cargas de trabalho que anteriormente exigiam sistemas distribu√≠dos separados, incluindo processamento em lote, consultas interativas, an√°lise de streaming, aprendizado de m√°quina e processamento de grafos." - Documenta√ß√£o Oficial do Apache Spark

### 1.2. A Evolu√ß√£o: Do Hadoop MapReduce ao Spark

Para entender a import√¢ncia do Spark, √© √∫til olhar para seu predecessor, o **Hadoop MapReduce**. O MapReduce foi pioneiro no processamento de Big Data, mas possu√≠a uma limita√ß√£o fundamental: ele dependia intensivamente de opera√ß√µes de leitura e escrita em disco. Cada etapa de um job MapReduce escrevia seus resultados intermedi√°rios no HDFS, o que gerava uma alta lat√™ncia, especialmente para algoritmos iterativos (como os de machine learning) e an√°lises interativas.

O Spark surgiu para superar essa limita√ß√£o. Sua principal inova√ß√£o foi a introdu√ß√£o do conceito de **Resilient Distributed Dataset (RDD)**, uma estrutura de dados que permite que os dados sejam mantidos em mem√≥ria entre as opera√ß√µes. Ao minimizar o I/O (Input/Output) de disco, o Spark alcan√ßou ganhos de performance de at√© 100 vezes em compara√ß√£o com o MapReduce para certas aplica√ß√µes.

| Caracter√≠stica | Hadoop MapReduce | Apache Spark |
| :--- | :--- | :--- |
| **Processamento** | Baseado em disco | Primariamente em mem√≥ria |
| **Lat√™ncia** | Alta | Baixa |
| **Velocidade** | Lento para cargas de trabalho iterativas | Muito r√°pido para cargas de trabalho iterativas e interativas |
| **API** | API de baixo n√≠vel (Map, Reduce) | APIs de alto n√≠vel (DataFrames, SQL, Streaming) |
| **Ecossistema** | Requer diferentes ferramentas para diferentes tarefas | Plataforma unificada para m√∫ltiplas tarefas |

### 1.3. O Ecossistema Unificado do Spark

Uma das maiores for√ßas do Spark √© seu ecossistema coeso, que oferece bibliotecas integradas para diversas necessidades de an√°lise de dados. Isso permite que os desenvolvedores construam aplica√ß√µes complexas de ponta a ponta usando uma √∫nica plataforma.

Os principais componentes do ecossistema Spark s√£o:

*   **Spark Core**: √â a base da plataforma. Ele fornece a funcionalidade principal do Spark, incluindo o agendamento de tarefas, gerenciamento de mem√≥ria, tratamento de falhas e a API de RDDs.

*   **Spark SQL**: Constru√≠do sobre o Spark Core, o Spark SQL introduz a abstra√ß√£o de **DataFrame** e permite a consulta de dados estruturados e semi-estruturados usando a sintaxe SQL. √â um dos componentes mais utilizados do Spark.

*   **Spark Streaming e Structured Streaming**: Bibliotecas para o processamento de fluxos de dados em tempo real. O **Structured Streaming**, a API mais recente, trata os fluxos de dados como tabelas que s√£o continuamente atualizadas, simplificando enormemente o desenvolvimento de aplica√ß√µes de streaming.

*   **MLlib (Machine Learning Library)**: A biblioteca de machine learning do Spark, que oferece uma ampla gama de algoritmos de classifica√ß√£o, regress√£o, clusteriza√ß√£o e filtragem colaborativa, al√©m de ferramentas para constru√ß√£o de pipelines de ML em escala.

*   **GraphX**: Uma API para processamento de grafos e computa√ß√£o paralela de grafos, √∫til para an√°lises de redes sociais, sistemas de recomenda√ß√£o e modelagem de redes.

### 1.4. Vantagens e Casos de Uso

O Apache Spark se tornou a ferramenta padr√£o para muitos cen√°rios de Big Data devido √†s suas vantagens distintas:

**Vantagens:**

*   **Velocidade**: Gra√ßas ao processamento em mem√≥ria e ao otimizador de consultas avan√ßado (Catalyst), o Spark √© extremamente r√°pido.
*   **Facilidade de Uso**: Oferece APIs de alto n√≠vel e expressivas em Python (PySpark), Scala, Java e R, tornando o desenvolvimento mais produtivo.
*   **Plataforma Unificada**: Combina processamento em lote, streaming, SQL e machine learning em uma √∫nica ferramenta, reduzindo a complexidade do sistema.
*   **Flexibilidade**: Pode ser executado em diversos ambientes (Hadoop YARN, Kubernetes, Mesos, Standalone) e se conectar a in√∫meras fontes de dados.

**Casos de Uso Comuns:**

*   **Engenharia de Dados e ETL**: Processamento e transforma√ß√£o de grandes volumes de dados (ETL - Extract, Transform, Load) de forma r√°pida e escal√°vel.
*   **An√°lise de Dados Interativa**: Analistas podem explorar terabytes de dados de forma interativa usando Spark SQL e notebooks (como o Colab).
*   **Machine Learning em Larga Escala**: Treinamento de modelos de machine learning em datasets que n√£o cabem em uma √∫nica m√°quina.
*   **Processamento de Dados em Tempo Real**: An√°lise de dados de streaming para detec√ß√£o de fraudes, monitoramento de sistemas, personaliza√ß√£o em tempo real e an√°lise de dados de IoT.

No pr√≥ximo cap√≠tulo, vamos mergulhar na arquitetura do Spark para entender como ele alcan√ßa essa performance e escalabilidade e velocidade.
# M√≥dulo 1: Fundamentos do Apache Spark

## Cap√≠tulo 2: Arquitetura do Apache Spark

### 2.1. Vis√£o Geral da Arquitetura

Para dominar o Spark, √© essencial compreender sua arquitetura. O Spark opera em um modelo de cluster mestre-escravo (master-slave), onde um n√≥ central coordena o trabalho que √© distribu√≠do entre v√°rios n√≥s de trabalho. Essa arquitetura √© o que permite ao Spark processar dados em paralelo e em grande escala.

Uma aplica√ß√£o Spark √© composta por dois tipos principais de processos:

1.  **Programa Driver (Driver Program)**: O processo mestre.
2.  **Executores (Executors)**: Os processos de trabalho (escravos).

Esses processos s√£o gerenciados por um **Gerenciador de Cluster (Cluster Manager)**, que √© respons√°vel por alocar os recursos do cluster para a aplica√ß√£o.

![Arquitetura do Spark](https://spark.apache.org/docs/latest/img/cluster-overview.png)
*Fonte: Documenta√ß√£o Oficial do Apache Spark*

### 2.2. Os Componentes em Detalhe

#### **Programa Driver (Driver Program)**

O Driver √© o cora√ß√£o da sua aplica√ß√£o Spark. √â o processo onde o m√©todo `main()` da sua aplica√ß√£o √© executado. Suas principais responsabilidades s√£o:

*   **Criar o `SparkContext`**: O `SparkContext` (ou a `SparkSession`, que o encapsula) √© a conex√£o principal com o cluster Spark.
*   **Analisar, Otimizar e Planejar**: O Driver pega o c√≥digo do usu√°rio (as transforma√ß√µes e a√ß√µes) e o converte em um plano de execu√ß√£o f√≠sico. Ele cria um **Grafo Ac√≠clico Dirigido (DAG)** de opera√ß√µes.
*   **Agendar Tarefas**: O Driver divide o plano de execu√ß√£o em unidades menores de trabalho chamadas **tarefas (tasks)** e as envia para os executores.
*   **Coordenar a Execu√ß√£o**: Ele monitora o progresso das tarefas e reagenda as que falham.

Em um ambiente como o Google Colab, o pr√≥prio notebook Python atua como o Programa Driver.

#### **Executores (Executors)**

Os Executores s√£o os processos de trabalho que rodam nos n√≥s do cluster (worker nodes). Eles s√£o lan√ßados no in√≠cio de uma aplica√ß√£o Spark e permanecem ativos durante toda a sua dura√ß√£o. Cada executor tem duas fun√ß√µes principais:

1.  **Executar Tarefas**: Eles recebem as tarefas do Driver e as executam nos dados. Cada tarefa opera em uma parti√ß√£o espec√≠fica dos dados.
2.  **Armazenar Dados**: Eles armazenam em cache (em mem√≥ria ou em disco) as parti√ß√µes de dados que foram persistidas pelo usu√°rio atrav√©s de `cache()` ou `persist()`.

Cada executor possui um n√∫mero alocado de **cores** (ou slots de tarefas), que determina quantas tarefas ele pode executar simultaneamente. Por exemplo, um executor com 4 cores pode executar 4 tarefas em paralelo.

#### **Gerenciador de Cluster (Cluster Manager)**

O Gerenciador de Cluster √© respons√°vel por alocar os recursos f√≠sicos (CPU, mem√≥ria) do cluster para as aplica√ß√µes Spark. O Spark √© agn√≥stico em rela√ß√£o ao gerenciador de cluster e pode ser executado em v√°rios deles:

*   **Standalone**: Um gerenciador de cluster simples que vem com o pr√≥prio Spark. √â f√°cil de configurar e √≥timo para come√ßar.
*   **Apache Hadoop YARN**: O gerenciador de recursos do Hadoop. √â a escolha mais comum em ambientes de produ√ß√£o que j√° possuem um ecossistema Hadoop.
*   **Apache Mesos**: Um gerenciador de cluster de prop√≥sito geral que pode executar diversas aplica√ß√µes, incluindo Spark.
*   **Kubernetes**: Um sistema de orquestra√ß√£o de cont√™ineres que se tornou uma op√ß√£o popular para executar o Spark, especialmente em ambientes de nuvem.

Quando voc√™ executa o Spark no modo `local[*]`, como fazemos no Colab, voc√™ est√° usando um modo especial onde o Driver e um Executor rodam dentro do mesmo processo na mesma m√°quina, simulando um ambiente de cluster.

### 2.3. O Fluxo de Execu√ß√£o de uma Aplica√ß√£o Spark

Vamos visualizar o ciclo de vida de uma aplica√ß√£o Spark:

1.  **Submiss√£o**: Voc√™ submete sua aplica√ß√£o (por exemplo, um script Python) ao Spark.
2.  **In√≠cio do Driver**: O Gerenciador de Cluster aloca recursos para o Programa Driver e o inicia.
3.  **Cria√ß√£o do `SparkContext`**: O Driver cria o `SparkContext`, que se conecta ao Gerenciador de Cluster.
4.  **Aloca√ß√£o dos Executores**: O `SparkContext` solicita recursos ao Gerenciador de Cluster para os Executores.
5.  **Lan√ßamento dos Executores**: O Gerenciador de Cluster lan√ßa os processos Executores nos n√≥s de trabalho.
6.  **Execu√ß√£o do C√≥digo**: O Driver executa o c√≥digo do usu√°rio. Quando uma **a√ß√£o** √© encontrada, o Driver cria o DAG, otimiza-o e o divide em est√°gios e tarefas.
7.  **Agendamento de Tarefas**: O Driver envia as tarefas para os Executores.
8.  **Execu√ß√£o das Tarefas**: Os Executores executam as tarefas em suas parti√ß√µes de dados e retornam os resultados para o Driver (se necess√°rio).
9.  **Fim da Aplica√ß√£o**: Quando a aplica√ß√£o termina, o `SparkContext` √© parado e o Gerenciador de Cluster libera todos os recursos.

### 2.4. O Grafo Ac√≠clico Dirigido (DAG)

O DAG √© um conceito central para a performance do Spark. Em vez de executar as opera√ß√µes uma a uma, o Spark constr√≥i um grafo de todas as transforma√ß√µes que voc√™ define. Este grafo √© "ac√≠clico" porque as opera√ß√µes fluem em uma √∫nica dire√ß√£o, e "dirigido" porque cada opera√ß√£o depende da anterior.

Quando uma a√ß√£o √© chamada, o **DAG Scheduler** entra em a√ß√£o. Ele olha para o grafo e o divide em **est√°gios (stages)**. Um est√°gio √© um conjunto de transforma√ß√µes que podem ser executadas juntas, sem a necessidade de um **shuffle** (a custosa opera√ß√£o de redistribuir dados pela rede). Uma nova etapa √© criada sempre que os dados precisam ser embaralhados.

Essa abordagem permite que o Spark otimize a execu√ß√£o, combinando opera√ß√µes e minimizando a movimenta√ß√£o de dados, o que √© fundamental para seu desempenho.

Compreender essa arquitetura √© o primeiro passo para escrever aplica√ß√µes PySpark eficientes e para ser capaz de diagnosticar e otimizar problemas de performance, um t√≥pico que exploraremos em detalhes em um m√≥dulo posterior.
# M√≥dulo 2: Opera√ß√µes com DataFrames

## Laborat√≥rio 1: Manipula√ß√£o B√°sica de DataFrames

### 1. Objetivo

Neste primeiro laborat√≥rio, vamos nos familiarizar com as opera√ß√µes mais fundamentais e essenciais para trabalhar com DataFrames no PySpark. Usaremos os datasets que voc√™ forneceu (`clientes.csv`, `produtos.csv`, `vendas.csv`) para realizar tarefas de explora√ß√£o, sele√ß√£o, filtragem e manipula√ß√£o de dados. O objetivo √© construir uma base s√≥lida para as an√°lises mais complexas que faremos nos pr√≥ximos m√≥dulos.

### 2. Pr√©-requisitos

*   Ambiente PySpark 4.0 configurado no Google Colab (conforme o M√≥dulo 0).
*   `SparkSession` iniciada.
*   Arquivos `clientes.csv`, `produtos.csv`, e `vendas.csv` carregados no ambiente do Colab.

### 3. Carregando e Inspecionando os Dados

Vamos come√ßar carregando nossos arquivos CSV em DataFrames e realizando uma inspe√ß√£o inicial para entender sua estrutura.

```python
# Certifique-se de que sua SparkSession est√° ativa
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").appName("CursoHadoop_Lab1").getOrCreate()

# Carregar os datasets
clientes_df = spark.read.csv('clientes.csv', header=True, inferSchema=True)
produtos_df = spark.read.csv('produtos.csv', header=True, inferSchema=True)
vendas_df = spark.read.csv('vendas.csv', header=True, inferSchema=True)
```

#### **Inspe√ß√£o B√°sica**

As primeiras a√ß√µes que realizamos em um novo DataFrame s√£o para entender sua forma e conte√∫do.

*   **`printSchema()`**: Mostra a "planta" do seu DataFrame, exibindo os nomes das colunas, seus tipos de dados e se permitem valores nulos.
*   **`show(n)`**: Exibe as primeiras `n` linhas do DataFrame em um formato de tabela. √â a maneira mais r√°pida de ter uma no√ß√£o dos dados.
*   **`count()`**: Retorna o n√∫mero total de linhas no DataFrame. Esta √© uma **a√ß√£o**, o que significa que o Spark executar√° todo o processamento necess√°rio para calcular o resultado.

```python
# Inspecionando o DataFrame de Vendas
print("Schema do DataFrame de Vendas:")
vendas_df.printSchema()

print("\nExibindo as 5 primeiras vendas:")
vendas_df.show(5)

print(f"\nN√∫mero total de registros de vendas: {vendas_df.count()}")

# Inspecionando o DataFrame de Clientes
print("\nSchema do DataFrame de Clientes:")
clientes_df.printSchema()

print(f"\nN√∫mero total de clientes: {clientes_df.count()}")
```

### 4. Selecionando Colunas (`select`)

A opera√ß√£o `select` √© usada para escolher uma ou mais colunas de um DataFrame, criando um novo DataFrame com apenas as colunas selecionadas. √â an√°loga √† cl√°usula `SELECT` do SQL.

```python
# Selecionar colunas espec√≠ficas do DataFrame de vendas
vendas_info_basica_df = vendas_df.select("venda_id", "cliente_id", "data_venda", "preco_total")

print("DataFrame de vendas com colunas selecionadas:")
vendas_info_basica_df.show(5)

# Voc√™ tamb√©m pode passar os nomes das colunas como strings separadas
produtos_info_df = produtos_df.select("nome_produto", "categoria", "preco")
produtos_info_df.show(5)
```

### 5. Filtrando Dados (`filter` e `where`)

Filtrar √© uma das opera√ß√µes mais comuns. Voc√™ usa `filter()` ou `where()` (s√£o sin√¥nimos) para selecionar linhas que atendem a uma determinada condi√ß√£o. As condi√ß√µes s√£o expressas usando a nota√ß√£o de colunas.

```python
from pyspark.sql.functions import col

# Filtrar vendas com pre√ßo total acima de 5000
vendas_caras_df = vendas_df.filter(col("preco_total") > 5000)

print(f"N√∫mero de vendas com pre√ßo total > 5000: {vendas_caras_df.count()}")
vendas_caras_df.show(5)

# Filtrar produtos da categoria "Eletr√¥nicos"
# Usando a sintaxe de string para a condi√ß√£o
produtos_eletronicos_df = produtos_df.filter("categoria = 'Eletr√¥nicos'")

print("\nProdutos da categoria Eletr√¥nicos:")
produtos_eletronicos_df.show(5)

# Combinando m√∫ltiplas condi√ß√µes
# Queremos clientes do estado de S√£o Paulo (SP) ou Rio de Janeiro (RJ)
clientes_sp_rj_df = clientes_df.filter(
    (col("estado") == "SP") | (col("estado") == "RJ")
)

print(f"\nN√∫mero de clientes de SP ou RJ: {clientes_sp_rj_df.count()}")
clientes_sp_rj_df.show(5)
```

### 6. Criando e Renomeando Colunas (`withColumn` e `withColumnRenamed`)

*   **`withColumn(nome_nova_coluna, expressao)`**: Adiciona uma nova coluna a um DataFrame ou substitui uma existente. A `expressao` define como os valores da nova coluna s√£o calculados.
*   **`withColumnRenamed(nome_antigo, nome_novo)`**: Simplesmente renomeia uma coluna existente.

Vamos usar `withColumn` para calcular o pre√ßo m√©dio por item em cada venda.

```python
# Calcular o pre√ßo m√©dio por item na venda
vendas_com_preco_medio_df = vendas_df.withColumn(
    "preco_medio_item", 
    col("preco_total") / col("quantidade")
)

print("Vendas com a nova coluna 'preco_medio_item':")
vendas_com_preco_medio_df.select("venda_id", "quantidade", "preco_total", "preco_medio_item").show(5)

# Renomear a coluna 'preco' em produtos_df para 'preco_unitario'
produtos_renomeado_df = produtos_df.withColumnRenamed("preco", "preco_unitario")

print("\nDataFrame de produtos com coluna renomeada:")
produtos_renomeado_df.printSchema()
```

### 7. Ordenando os Dados (`orderBy`)

A opera√ß√£o `orderBy` (ou `sort`) √© usada para ordenar as linhas de um DataFrame com base em uma ou mais colunas.

```python
from pyspark.sql.functions import desc, asc

# Encontrar as 10 vendas mais caras
print("Top 10 vendas mais caras:")
vendas_df.orderBy(desc("preco_total")).show(10)

# Encontrar os 10 produtos mais baratos
print("\nTop 10 produtos mais baratos:")
produtos_df.orderBy(col("preco").asc()).show(10) # .asc() √© o padr√£o, mas √© bom ser expl√≠cito
```

### 8. Removendo Colunas (`drop`)

Se voc√™ precisar remover colunas de um DataFrame, pode usar o m√©todo `drop()`.

```python
# No DataFrame de clientes, a coluna 'email' pode n√£o ser necess√°ria para uma an√°lise agregada
clientes_sem_email_df = clientes_df.drop("email")

print("DataFrame de clientes sem a coluna 'email':")
clientes_sem_email_df.printSchema()
```

### 9. Conclus√£o do Laborat√≥rio

Parab√©ns! Voc√™ completou o primeiro laborat√≥rio pr√°tico. Agora voc√™ est√° familiarizado com as opera√ß√µes mais essenciais para inspecionar, selecionar, filtrar e transformar DataFrames no PySpark. Essas s√£o as ferramentas que voc√™ usar√° repetidamente em qualquer an√°lise de dados.

No pr√≥ximo laborat√≥rio, vamos explorar opera√ß√µes mais avan√ßadas, como agrega√ß√µes (`groupBy`) e jun√ß√µes (`join`), para come√ßar a cruzar informa√ß√µes entre nossos diferentes datasets.
# M√≥dulo 2: Opera√ß√µes com DataFrames

## Laborat√≥rio 2: Agrega√ß√µes e Jun√ß√µes (`groupBy` e `join`)

### 1. Objetivo

No laborat√≥rio anterior, aprendemos a manipular um √∫nico DataFrame. Agora, vamos dar um passo adiante e explorar duas das opera√ß√µes mais poderosas e fundamentais em qualquer sistema de an√°lise de dados: **agrega√ß√µes** e **jun√ß√µes**. O verdadeiro poder do Spark (e do SQL em geral) √© revelado quando come√ßamos a cruzar informa√ß√µes de diferentes fontes de dados para gerar insights de neg√≥cio.

Neste laborat√≥rio, vamos responder a perguntas de neg√≥cio importantes usando os datasets de clientes, produtos e vendas:

*   Qual o valor total de vendas por categoria de produto?
*   Quais s√£o os produtos mais vendidos (em valor e em quantidade)?
*   Quais s√£o os clientes que mais compram em nossa loja?

Para isso, vamos mergulhar nos conceitos de `groupBy()` e `join()`.

### 2. Agrega√ß√µes com `groupBy()`

A opera√ß√£o `groupBy()` √© usada para agrupar linhas que t√™m os mesmos valores em uma ou mais colunas. Depois de agrupar os dados, voc√™ pode aplicar uma ou mais **fun√ß√µes de agrega√ß√£o** para calcular uma m√©trica resumida para cada grupo.

**Como funciona?**

1.  **Fase de Shuffle (Embaralhamento)**: O Spark redistribui os dados pela rede. Ele garante que todas as linhas com a mesma chave de agrupamento (ex: a mesma `categoria` de produto) terminem na mesma parti√ß√£o, no mesmo n√≥ executor.
2.  **Fase de Agrega√ß√£o**: Em cada parti√ß√£o, o Spark aplica a fun√ß√£o de agrega√ß√£o (ex: `sum()`, `count()`, `avg()`) a todas as linhas dentro de cada grupo.

Principais Fun√ß√µes de Agrega√ß√£o:

*   `sum()`: Calcula a soma de uma coluna num√©rica.
*   `count()`: Conta o n√∫mero de linhas em um grupo.
*   `avg()`: Calcula a m√©dia de uma coluna num√©rica.
*   `min()`: Encontra o valor m√≠nimo em um grupo.
*   `max()`: Encontra o valor m√°ximo em um grupo.

#### **Exemplo Pr√°tico: Total de Vendas e Quantidade por Cliente**

Vamos usar apenas o DataFrame de `vendas_df` para descobrir o valor total e a quantidade total de itens comprados por cada cliente.

```python
from pyspark.sql.functions import sum, count, avg, desc

# Carregar o DataFrame de vendas se ainda n√£o o fez
vendas_df = spark.read.csv('vendas.csv', header=True, inferSchema=True)

# Agrupar por 'cliente_id' e aplicar as agrega√ß√µes
analise_cliente_df = vendas_df.groupBy("cliente_id") \
    .agg(
        sum("preco_total").alias("valor_total_gasto"),
        sum("quantidade").alias("quantidade_total_itens"),
        count("venda_id").alias("numero_de_compras")
    )

print("An√°lise de gastos por cliente:")
# Ordenar pelos clientes que mais gastaram
analise_cliente_df.orderBy(desc("valor_total_gasto")).show(10)
```

Neste c√≥digo, `.agg()` √© o m√©todo que nos permite aplicar m√∫ltiplas fun√ß√µes de agrega√ß√£o de uma s√≥ vez. Usamos `.alias()` para dar nomes mais descritivos √†s novas colunas agregadas.

### 3. Jun√ß√µes com `join()`

A opera√ß√£o `join()` √© usada para combinar colunas de dois DataFrames com base em uma condi√ß√£o de jun√ß√£o. √â a maneira como cruzamos informa√ß√µes, por exemplo, para descobrir o nome de um cliente a partir de seu `cliente_id` em uma venda.

**Como funciona?**

O `join` tamb√©m envolve uma fase de **shuffle**, onde o Spark redistribui os dados de ambos os DataFrames para que as linhas com a mesma chave de jun√ß√£o (ex: o mesmo `cliente_id`) fiquem no mesmo executor. Uma vez que os dados est√£o co-localizados, o Spark pode realizar a combina√ß√£o.

#### **Tipos de Join**

O PySpark suporta todos os tipos de join padr√£o do SQL. O tipo de join determina como lidar com as chaves que existem em um DataFrame, mas n√£o no outro.

| Tipo de Join | Descri√ß√£o | Sintaxe PySpark |
| :--- | :--- | :--- |
| **`inner`** | (Padr√£o) Retorna apenas as linhas onde a chave de jun√ß√£o existe em **ambos** os DataFrames. | `how='inner'` |
| **`left`** | Retorna **todas** as linhas do DataFrame da esquerda e as linhas correspondentes do da direita. Se n√£o houver correspond√™ncia, as colunas da direita ficam nulas. | `how='left'` |
| **`right`** | O oposto do `left`. Retorna **todas** as linhas do DataFrame da direita. | `how='right'` |
| **`full_outer`** | Retorna **todas** as linhas quando h√° uma correspond√™ncia em um dos DataFrames. Se n√£o houver correspond√™ncia, as colunas do outro DataFrame ficam nulas. | `how='full_outer'` |

### 4. Laborat√≥rio Pr√°tico: Cruzando Dados de Vendas, Clientes e Produtos

Agora vamos combinar tudo para responder √†s nossas perguntas de neg√≥cio.

#### **An√°lise 1: Clientes que Mais Compram**

Vamos juntar nossa an√°lise de clientes com o DataFrame de clientes para ver os nomes dos top compradores.

```python
# Carregar os DataFrames
clientes_df = spark.read.csv('clientes.csv', header=True, inferSchema=True)
vendas_df = spark.read.csv('vendas.csv', header=True, inferSchema=True)

# 1. Agrupar as vendas por cliente
analise_cliente_df = vendas_df.groupBy("cliente_id") \
    .agg(
        sum("preco_total").alias("valor_total_gasto"),
        count("venda_id").alias("numero_de_compras")
    )

# 2. Juntar o resultado com o DataFrame de clientes
# A condi√ß√£o de join √© a coluna 'cliente_id'
top_clientes_df = analise_cliente_df.join(
    clientes_df, 
    analise_cliente_df.cliente_id == clientes_df.cliente_id, 
    'inner'
)

print("--- Top 10 Clientes por Valor Gasto ---")
top_clientes_df \
    .select("nome", "cidade", "estado", "valor_total_gasto", "numero_de_compras") \
    .orderBy(desc("valor_total_gasto")) \
    .show(10)
```

#### **An√°lise 2: Vendas por Categoria e Produto**

Para esta an√°lise, precisamos juntar os DataFrames de vendas e produtos.

```python
# Carregar os DataFrames
produtos_df = spark.read.csv('produtos.csv', header=True, inferSchema=True)
vendas_df = spark.read.csv('vendas.csv', header=True, inferSchema=True)

# 1. Juntar vendas com produtos usando 'produto_id'
vendas_produtos_df = vendas_df.join(produtos_df, "produto_id", "inner")

print("DataFrame combinado de Vendas e Produtos:")
vendas_produtos_df.show(5)

# 2. An√°lise: Vendas totais por Categoria

vendas_por_categoria_df = vendas_produtos_df.groupBy("categoria") \
    .agg(
        sum("preco_total").alias("faturamento_total"),
        sum("quantidade").alias("itens_vendidos")
    )

print("\n--- Faturamento Total por Categoria ---")
vendas_por_categoria_df.orderBy(desc("faturamento_total")).show()

# 3. An√°lise: Produtos mais vendidos

vendas_por_produto_df = vendas_produtos_df.groupBy("nome_produto", "categoria") \
    .agg(
        sum("preco_total").alias("faturamento_total"),
        sum("quantidade").alias("itens_vendidos")
    )

print("\n--- Top 10 Produtos Mais Vendidos (por faturamento) ---")
vendas_por_produto_df.orderBy(desc("faturamento_total")).show(10)

print("\n--- Top 10 Produtos Mais Vendidos (por quantidade) ---")
vendas_por_produto_df.orderBy(desc("itens_vendidos")).show(10)
```

### 5. Conclus√£o do Laborat√≥rio

Neste laborat√≥rio, voc√™ aprendeu a usar `groupBy` para agregar dados e `join` para combinar diferentes fontes de informa√ß√£o. Essas duas opera√ß√µes s√£o a espinha dorsal da maioria das an√°lises de dados.

Voc√™ foi capaz de transformar dados brutos de vendas, clientes e produtos em insights acion√°veis, como identificar seus clientes e produtos mais valiosos. Dominar `groupBy` e `join` √© um passo crucial para se tornar proficiente em PySpark.

No pr√≥ximo m√≥dulo, vamos explorar como podemos expressar essas mesmas l√≥gicas usando a sintaxe familiar do **Spark SQL**.
# M√≥dulo 3: An√°lise de Dados com Spark SQL

## Cap√≠tulo 1: Introdu√ß√£o ao Spark SQL

### 1.1. O que √© Spark SQL?

**Spark SQL** √© um dos m√≥dulos mais poderosos e amplamente utilizados do ecossistema Apache Spark. Ele foi projetado para o processamento de dados estruturados e semi-estruturados, trazendo duas contribui√ß√µes fundamentais para o Spark:

1.  **A API de DataFrames e Datasets**: Uma interface de alto n√≠vel para trabalhar com dados de forma tabular, que √© mais otimizada e, para muitos, mais intuitiva do que a API de RDDs.
2.  **Um motor SQL completo**: Permite que voc√™ execute consultas SQL padr√£o diretamente sobre seus DataFrames ou sobre fontes de dados externas.

Essencialmente, o Spark SQL preenche a lacuna entre o mundo do SQL tradicional e a an√°lise de dados em larga escala, permitindo que analistas de dados, engenheiros e cientistas de dados usem suas habilidades existentes em SQL para explorar e processar Big Data.

> O Spark SQL permite que os usu√°rios do Spark aproveitem o poder do processamento distribu√≠do e em mem√≥ria do Spark usando uma sintaxe SQL familiar. Ele se integra perfeitamente com a API de DataFrames, permitindo que voc√™ misture consultas SQL com manipula√ß√µes program√°ticas de DataFrames na mesma aplica√ß√£o.

### 1.2. A M√°gica por Tr√°s do Spark SQL: Catalyst Optimizer

O cora√ß√£o do Spark SQL e da API de DataFrames √© o **Catalyst Optimizer**. O Catalyst √© um otimizador de consultas extens√≠vel, baseado em √°rvores, que realiza uma s√©rie de otimiza√ß√µes l√≥gicas e f√≠sicas no seu c√≥digo antes de execut√°-lo.

Quando voc√™ escreve uma consulta SQL ou uma opera√ß√£o de DataFrame, o Catalyst a converte em uma √°rvore de plano l√≥gico. Em seguida, ele aplica um conjunto de regras para otimizar esse plano, como:

*   **Predicate Pushdown**: Empurra as opera√ß√µes de filtro (`WHERE`) para o mais perto poss√≠vel da fonte de dados. Isso reduz drasticamente a quantidade de dados que precisam ser lidos e processados nas etapas subsequentes.
*   **Column Pruning**: Remove colunas que n√£o s√£o necess√°rias para a consulta final, minimizando a quantidade de dados lidos e transferidos pela rede.
*   **Join Reordering**: Reordena as jun√ß√µes para garantir que as menores tabelas sejam processadas primeiro, reduzindo a quantidade de dados embaralhados (shuffled).

Ap√≥s a otimiza√ß√£o l√≥gica, o Catalyst gera m√∫ltiplos planos f√≠sicos e escolhe o mais eficiente com base em um modelo de custos. O resultado final √© um c√≥digo RDD altamente otimizado que √© executado no cluster.

√â gra√ßas ao Catalyst que o c√≥digo DataFrame/SQL √© frequentemente mais r√°pido do que o c√≥digo RDD escrito manualmente, pois ele aplica otimiza√ß√µes complexas automaticamente.

### 1.3. Executando Consultas SQL no PySpark

Existem duas maneiras principais de executar consultas SQL no PySpark:

1.  **`spark.sql()`**: Este m√©todo, dispon√≠vel no objeto `SparkSession`, permite executar qualquer consulta SQL como uma string. Ele retorna o resultado como um novo DataFrame.
2.  **Tabelas Tempor√°rias (Temporary Views)**: Para que o Spark SQL possa "ver" um DataFrame, voc√™ precisa registr√°-lo como uma tabela tempor√°ria. Uma "view" n√£o armazena os dados; √© apenas um ponteiro para o DataFrame original. Uma vez registrada, voc√™ pode referenciar essa view em suas consultas SQL.

#### **Criando uma Tabela Tempor√°ria**

Voc√™ pode criar uma view tempor√°ria usando o m√©todo `createOrReplaceTempView()` em um DataFrame.

```python
# Supondo que 'vendas_df' √© um DataFrame existente
vendas_df.createOrReplaceTempView("vendas_view")

# Agora voc√™ pode usar "vendas_view" em suas consultas SQL
resultado_df = spark.sql("SELECT * FROM vendas_view WHERE quantidade > 5")
```

**Diferen√ßa entre `createOrReplaceTempView` e `createGlobalTempView`**

| M√©todo | Escopo | Vida √ötil |
| :--- | :--- | :--- |
| `createOrReplaceTempView` | Associado √† `SparkSession` atual. | A view √© descartada quando a `SparkSession` termina. |
| `createGlobalTempView` | Acess√≠vel por todas as `SparkSession`s no mesmo cluster. | A view persiste at√© que a aplica√ß√£o Spark termine. Deve ser acessada com o prefixo `global_temp`. |

Para a maioria dos casos de uso, especialmente em notebooks, `createOrReplaceTempView` √© suficiente e mais simples de gerenciar.

### 1.4. Vantagens de Usar Spark SQL

*   **Performance**: Gra√ßas ao Catalyst Optimizer, as consultas SQL s√£o convertidas em planos de execu√ß√£o altamente eficientes.
*   **Familiaridade e Produtividade**: Permite que qualquer pessoa com conhecimento de SQL comece a trabalhar com Big Data imediatamente, sem precisar aprender uma nova API program√°tica complexa.
*   **Interoperabilidade**: Voc√™ pode misturar e combinar perfeitamente a API de DataFrames com consultas SQL na mesma aplica√ß√£o. O resultado de uma consulta SQL √© um DataFrame, que pode ser manipulado posteriormente com a API program√°tica, e vice-versa.
*   **Conectividade**: O Spark SQL pode se conectar a uma ampla variedade de fontes de dados (JDBC, Parquet, JSON, etc.) e expor esses dados para consulta via SQL.

No pr√≥ximo laborat√≥rio, vamos colocar o Spark SQL em pr√°tica. Vamos recriar as an√°lises do laborat√≥rio anterior (vendas por categoria, top clientes) usando apenas consultas SQL, para que voc√™ possa ver o poder e a simplicidade do Spark SQL em a√ß√£o.
# M√≥dulo 3: An√°lise de Dados com Spark SQL

## Laborat√≥rio: An√°lise de Vendas com Spark SQL

### 1. Objetivo

Neste laborat√≥rio, vamos revisitar as an√°lises que fizemos no m√≥dulo anterior, mas desta vez, usaremos exclusivamente o poder do **Spark SQL**. O objetivo √© demonstrar como voc√™ pode realizar as mesmas opera√ß√µes de agrega√ß√£o e jun√ß√£o usando a sintaxe SQL que voc√™ j√° conhece, diretamente no PySpark.

Ao final deste laborat√≥rio, voc√™ ser√° capaz de:

*   Registrar DataFrames como tabelas tempor√°rias.
*   Executar consultas `SELECT`, `GROUP BY`, e `JOIN` usando `spark.sql()`.
*   Resolver problemas de neg√≥cio complexos combinando a simplicidade do SQL com o poder de processamento do Spark.

### 2. Preparando o Ambiente

Primeiro, vamos carregar nossos DataFrames e registr√°-los como views tempor√°rias para que o Spark SQL possa acess√°-los.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc

# Iniciar a SparkSession
spark = SparkSession.builder.master("local[*]").appName("CursoHadoop_LabSQL").getOrCreate()

# Carregar os datasets
clientes_df = spark.read.csv("clientes.csv", header=True, inferSchema=True)
produtos_df = spark.read.csv("produtos.csv", header=True, inferSchema=True)
vendas_df = spark.read.csv("vendas.csv", header=True, inferSchema=True)

# Criar views tempor√°rias para cada DataFrame
clientes_df.createOrReplaceTempView("clientes")
produtos_df.createOrReplaceTempView("produtos")
vendas_df.createOrReplaceTempView("vendas")

print("DataFrames carregados e views tempor√°rias criadas com sucesso!")
```

### 3. An√°lise 1: Clientes que Mais Compram (SQL)

No laborat√≥rio anterior, usamos `groupBy` e `join` na API de DataFrames para encontrar os clientes que mais gastaram. Agora, vamos fazer o mesmo com uma √∫nica consulta SQL.

**L√≥gica da Consulta:**

1.  **`FROM vendas`**: Come√ßamos com a tabela de vendas.
2.  **`JOIN clientes ON vendas.cliente_id = clientes.cliente_id`**: Juntamos as vendas com os clientes para obter os nomes e detalhes dos clientes.
3.  **`GROUP BY clientes.cliente_id, clientes.nome, clientes.cidade, clientes.estado`**: Agrupamos por cliente para calcular as m√©tricas de cada um.
4.  **`SELECT ... SUM(preco_total) AS valor_total_gasto, COUNT(venda_id) AS numero_de_compras`**: Calculamos o valor total gasto e o n√∫mero de compras para cada cliente.
5.  **`ORDER BY valor_total_gasto DESC`**: Ordenamos o resultado para ver os melhores clientes no topo.

```python
query_top_clientes = """
    SELECT 
        c.nome,
        c.cidade,
        c.estado,
        SUM(v.preco_total) AS valor_total_gasto,
        COUNT(v.venda_id) AS numero_de_compras
    FROM vendas v
    JOIN clientes c ON v.cliente_id = c.cliente_id
    GROUP BY c.cliente_id, c.nome, c.cidade, c.estado
    ORDER BY valor_total_gasto DESC
"""

top_clientes_sql_df = spark.sql(query_top_clientes)

print("--- Top 10 Clientes por Valor Gasto (via SQL) ---")
top_clientes_sql_df.show(10)
```

Como voc√™ pode ver, o resultado √© id√™ntico ao que obtivemos com a API de DataFrames, mas a l√≥gica est√° contida em uma √∫nica consulta SQL, que pode ser mais leg√≠vel para quem tem experi√™ncia com bancos de dados.

### 4. An√°lise 2: Vendas por Categoria e Produto (SQL)

Agora, vamos analisar o desempenho dos produtos e categorias, novamente usando SQL.

**L√≥gica da Consulta (Vendas por Categoria):**

1.  **`FROM vendas`**: Come√ßamos com as vendas.
2.  **`JOIN produtos ON vendas.produto_id = produtos.produto_id`**: Juntamos com os produtos para obter a categoria de cada item vendido.
3.  **`GROUP BY produtos.categoria`**: Agrupamos por categoria.
4.  **`SELECT ... SUM(preco_total) AS faturamento_total, SUM(quantidade) AS itens_vendidos`**: Calculamos o faturamento e o total de itens vendidos por categoria.
5.  **`ORDER BY faturamento_total DESC`**: Ordenamos para ver as categorias mais lucrativas.

```python
# Consulta para faturamento por categoria
query_vendas_categoria = """
    SELECT
        p.categoria,
        SUM(v.preco_total) AS faturamento_total,
        SUM(v.quantidade) AS itens_vendidos
    FROM vendas v
    JOIN produtos p ON v.produto_id = p.produto_id
    GROUP BY p.categoria
    ORDER BY faturamento_total DESC
"""

vendas_categoria_sql_df = spark.sql(query_vendas_categoria)

print("\n--- Faturamento Total por Categoria (via SQL) ---")
vendas_categoria_sql_df.show()

# Consulta para produtos mais vendidos
query_top_produtos = """
    SELECT
        p.nome_produto,
        p.categoria,
        SUM(v.preco_total) AS faturamento_total,
        SUM(v.quantidade) AS itens_vendidos
    FROM vendas v
    JOIN produtos p ON v.produto_id = p.produto_id
    GROUP BY p.nome_produto, p.categoria
    ORDER BY faturamento_total DESC
"""

top_produtos_sql_df = spark.sql(query_top_produtos)

print("\n--- Top 10 Produtos Mais Vendidos (por faturamento, via SQL) ---")
top_produtos_sql_df.show(10)
```

### 5. Misturando SQL e a API de DataFrames

A verdadeira flexibilidade do PySpark vem da capacidade de misturar as duas abordagens. O resultado de `spark.sql()` √© um DataFrame, ent√£o voc√™ pode aplicar mais transforma√ß√µes nele usando a API program√°tica.

Por exemplo, vamos pegar o resultado da nossa consulta de top produtos e adicionar uma coluna que calcula o faturamento m√©dio por item vendido.

```python
# O resultado de spark.sql() √© um DataFrame
top_produtos_sql_df = spark.sql(query_top_produtos)

# Agora, usamos a API de DataFrames para adicionar uma nova coluna
produtos_com_media_df = top_produtos_sql_df.withColumn(
    "faturamento_medio_por_item",
    col("faturamento_total") / col("itens_vendidos")
)

print("\n--- Top Produtos com Faturamento M√©dio por Item ---")
produtos_com_media_df.show(10)
```

### 6. Conclus√£o

Neste laborat√≥rio, voc√™ viu como o Spark SQL oferece uma alternativa poderosa e expressiva √† API de DataFrames para realizar an√°lises complexas. Para muitas pessoas, escrever uma consulta SQL √© mais r√°pido e mais claro do que encadear m√∫ltiplas chamadas de m√©todos de DataFrame.

Voc√™ aprendeu a:

*   Registrar DataFrames como `TempViews`.
*   Executar consultas `SELECT`, `JOIN`, `GROUP BY`, e `ORDER BY`.
*   Combinar a simplicidade do SQL com a flexibilidade da API de DataFrames.

Com o dom√≠nio do Spark SQL e da API de DataFrames, voc√™ agora tem um conjunto de ferramentas completo para enfrentar praticamente qualquer desafio de an√°lise de dados estruturados no PySpark.

No pr√≥ximo m√≥dulo, vamos mudar nosso foco da an√°lise de dados para o **Machine Learning**, explorando como usar a biblioteca MLlib do Spark para construir modelos preditivos em larga escala.
# M√≥dulo 4: Machine Learning com Spark MLlib

## Cap√≠tulo 1: Introdu√ß√£o ao Spark MLlib

### 1.1. O que √© MLlib?

**MLlib** √© a biblioteca de Machine Learning (Aprendizado de M√°quina) do Apache Spark. Seu principal objetivo √© tornar o machine learning pr√°tico, escal√°vel e f√°cil de usar. Ela foi projetada para ser executada em paralelo em clusters, permitindo o treinamento de modelos em datasets massivos que n√£o caberiam na mem√≥ria de uma √∫nica m√°quina.

Desde o Spark 2.0, a API principal da MLlib √© baseada em **DataFrames**. Essa API, conhecida como `pyspark.ml`, oferece uma interface de alto n√≠vel e uniforme para a cria√ß√£o de modelos e pipelines de machine learning, que se integra perfeitamente com o resto do ecossistema Spark.

> A MLlib cont√©m implementa√ß√µes de alta qualidade e escal√°veis de algoritmos comuns de classifica√ß√£o, regress√£o, clusteriza√ß√£o e filtragem colaborativa, al√©m de utilit√°rios para engenharia de features, constru√ß√£o de pipelines e persist√™ncia de modelos.

### 1.2. O Paradigma da API `pyspark.ml`: Transformers, Estimators e Pipelines

A API de MLlib baseada em DataFrames √© constru√≠da em torno de tr√™s conceitos principais, que fornecem uma linguagem comum para a cria√ß√£o de fluxos de trabalho de machine learning:

#### **Transformer (Transformador)**

Um **Transformer** √© um algoritmo que pode transformar um DataFrame em outro. Geralmente, um Transformer adiciona uma ou mais colunas ao DataFrame de entrada. A l√≥gica da transforma√ß√£o √© implementada no m√©todo `transform()`.

*   **Exemplo**: Um modelo de machine learning treinado √© um Transformer. Ele recebe um DataFrame com as *features* (caracter√≠sticas) e o transforma em um novo DataFrame que inclui as *previs√µes*.
*   **Exemplo 2**: Um algoritmo de engenharia de features, como um que converte uma coluna de texto em um vetor num√©rico, tamb√©m √© um Transformer.

#### **Estimator (Estimador)**

Um **Estimator** √© um algoritmo que pode ser treinado (ou "ajustado") em um DataFrame para produzir um Transformer. A l√≥gica de treinamento √© implementada no m√©todo `fit()`.

*   **Exemplo**: Um algoritmo de classifica√ß√£o, como `LogisticRegression`, √© um Estimator. Voc√™ chama o m√©todo `fit()` em um DataFrame de treinamento (que cont√©m as features e os r√≥tulos verdadeiros), e ele retorna um `LogisticRegressionModel`, que √© um Transformer. Este modelo pode ent√£o ser usado para fazer previs√µes em novos dados.

| Conceito | O que faz? | M√©todo Principal | Exemplo |
| :--- | :--- | :--- | :--- |
| **Estimator** | Aprende a partir dos dados. | `fit()` | `LogisticRegression` (o algoritmo) |
| **Transformer** | Transforma os dados. | `transform()` | `LogisticRegressionModel` (o modelo treinado) |

#### **Pipeline (Fluxo de Trabalho)**

Um **Pipeline** encadeia m√∫ltiplos Transformers e Estimators para criar um fluxo de trabalho de machine learning completo e unificado. Um Pipeline √©, ele pr√≥prio, um Estimator.

Quando voc√™ chama `fit()` em um Pipeline, ele executa cada est√°gio em ordem. Se o est√°gio for um Estimator, ele chama `fit()` para trein√°-lo e obter um Transformer. Se o est√°gio j√° for um Transformer, ele simplesmente o passa para a pr√≥xima etapa. O resultado de `fit()` em um Pipeline √© um `PipelineModel`, que √© um Transformer.

Os Pipelines s√£o extremamente √∫teis porque automatizam todo o fluxo de trabalho, desde o pr√©-processamento dos dados e engenharia de features at√© o treinamento do modelo. Isso garante que as mesmas transforma√ß√µes sejam aplicadas de forma consistente tanto nos dados de treinamento quanto nos de teste.

### 1.3. Etapas Comuns em um Fluxo de Trabalho de MLlib

Um projeto t√≠pico de machine learning com PySpark MLlib segue estas etapas:

1.  **Prepara√ß√£o dos Dados**: Carregar os dados como um DataFrame e realizar a limpeza inicial.
2.  **Engenharia de Features (Feature Engineering)**: Usar uma s√©rie de **Transformers** para converter os dados brutos em *features* num√©ricas que o modelo possa entender. Isso inclui:
    *   `StringIndexer`: Converter colunas de string (categ√≥ricas) em √≠ndices num√©ricos.
    *   `OneHotEncoder`: Converter os √≠ndices num√©ricos em vetores one-hot.
    *   `VectorAssembler`: Agrupar m√∫ltiplas colunas de features em uma √∫nica coluna de vetor. **Esta √© uma etapa obrigat√≥ria**, pois os algoritmos da MLlib esperam uma √∫nica coluna de features do tipo vetor.
3.  **Divis√£o dos Dados**: Dividir o DataFrame em conjuntos de treinamento e teste usando `randomSplit()`.
4.  **Defini√ß√£o do Modelo**: Instanciar o **Estimator** do modelo que voc√™ deseja treinar (ex: `LinearRegression`, `DecisionTreeClassifier`).
5.  **Cria√ß√£o do Pipeline**: Montar um **Pipeline** com todos os est√°gios de engenharia de features e o Estimator do modelo.
6.  **Treinamento do Modelo**: Chamar `fit()` no Pipeline com o conjunto de treinamento. Isso executar√° todo o fluxo e retornar√° um `PipelineModel` treinado.
7.  **Realiza√ß√£o de Previs√µes**: Chamar `transform()` no `PipelineModel` com o conjunto de teste para gerar as previs√µes.
8.  **Avalia√ß√£o do Modelo**: Usar um dos **Evaluators** da MLlib (ex: `RegressionEvaluator`, `BinaryClassificationEvaluator`) para medir a performance do modelo nas previs√µes geradas.

### 1.4. Vantagens da Abordagem de Pipeline

*   **Consist√™ncia**: Garante que o pr√©-processamento seja aplicado da mesma forma nos dados de treino, teste e produ√ß√£o, evitando erros comuns.
*   **Simplicidade**: Simplifica o gerenciamento de fluxos de trabalho complexos com muitas etapas.
*   **Persist√™ncia**: Voc√™ pode salvar e carregar todo o Pipeline treinado (`PipelineModel`) em disco, o que facilita a implanta√ß√£o do modelo em produ√ß√£o.

No pr√≥ximo laborat√≥rio, vamos aplicar todos esses conceitos para construir nosso primeiro modelo de machine learning com PySpark, usando um dataset real da internet para prever um resultado de forma de pagamento.
# M√≥dulo 4: Machine Learning com Spark MLlib

## Laborat√≥rio: Construindo um Modelo de Classifica√ß√£o para Prever Formas de Pagamento

### 1. Objetivo

Neste laborat√≥rio, vamos construir nosso primeiro modelo de machine learning de ponta a ponta usando o PySpark. O objetivo √© prever qual a **forma de pagamento** (`payment_type`) um cliente de ecommerce ir√° usar, com base em outras caracter√≠sticas da transa√ß√£o.

Para isso, usaremos um dataset p√∫blico de ecommerce da Olist, dispon√≠vel no Kaggle. Este √© um problema de **classifica√ß√£o multiclasse**, pois a forma de pagamento pode ser `credit_card`, `boleto`, `voucher`, ou `debit_card`.

Vamos passar por todas as etapas de um projeto de ML:

1.  Carregar e explorar os dados.
2.  Realizar a engenharia de features para preparar os dados para o modelo.
3.  Construir um `Pipeline` de ML.
4.  Treinar e avaliar um modelo de `DecisionTreeClassifier` (√Årvore de Decis√£o).

### 2. Preparando o Dataset

Primeiro, precisamos baixar o dataset do Kaggle. Usaremos o dataset "Olist E-Commerce" e, especificamente, o arquivo `olist_order_payments_dataset.csv`.

**Passo 1: Baixar o Dataset**

Voc√™ pode baixar o arquivo diretamente da p√°gina do dataset no Kaggle:
[https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

Ap√≥s baixar, fa√ßa o upload do arquivo `olist_order_payments_dataset.csv` para o seu ambiente Google Colab.

**Passo 2: Carregar e Explorar os Dados**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").appName("CursoHadoop_LabML").getOrCreate()

# Carregar o dataset de pagamentos
payments_df = spark.read.csv("olist_order_payments_dataset.csv", header=True, inferSchema=True)

# Explorar os dados
print("Schema do Dataset de Pagamentos:")
payments_df.printSchema()

print("\nExibindo algumas linhas:")
payments_df.show(5)

# Ver a distribui√ß√£o das formas de pagamento (nossa vari√°vel alvo)
print("\nDistribui√ß√£o das Formas de Pagamento:")
payments_df.groupBy("payment_type").count().show()
```

### 3. Engenharia de Features

Nosso objetivo √© prever `payment_type`. Os algoritmos de MLlib precisam que a vari√°vel alvo (label) e as features (caracter√≠sticas) estejam em formato num√©rico.

Nosso plano de engenharia de features ser√°:

1.  **`StringIndexer` para a Vari√°vel Alvo**: Converter a coluna `payment_type` (string) em uma coluna num√©rica chamada `label`. Esta √© uma exig√™ncia da MLlib.
2.  **`VectorAssembler` para as Features**: Agrupar nossas features num√©ricas (`payment_sequential`, `payment_installments`, `payment_value`) em uma √∫nica coluna de vetor chamada `features`.

```python
from pyspark.ml.feature import StringIndexer, VectorAssembler

# Etapa 1: Converter a coluna alvo (string) para num√©rica (label)
# O StringIndexer atribui um √≠ndice num√©rico a cada categoria de string.
label_indexer = StringIndexer(inputCol="payment_type", outputCol="label")

# Etapa 2: Agrupar as colunas de features em um √∫nico vetor
# Nossas features s√£o as colunas que usaremos para fazer a previs√£o.
feature_cols = ["payment_sequential", "payment_installments", "payment_value"]
vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
```

### 4. Construindo o Pipeline e o Modelo

Agora que temos nossos est√°gios de pr√©-processamento, vamos definir nosso modelo e montar o `Pipeline`.

*   **Modelo**: Usaremos um `DecisionTreeClassifier`, um modelo simples e interpret√°vel, √≥timo para come√ßar.
*   **Pipeline**: Nosso pipeline ter√° 3 est√°gios: o `label_indexer`, o `vector_assembler`, e o `classifier`.

```python
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml import Pipeline

# Etapa 3: Definir o modelo (Estimator)
# labelCol="label" e featuresCol="features" s√£o os nomes padr√£o, mas √© bom ser expl√≠cito.
dt = DecisionTreeClassifier(labelCol="label", featuresCol="features")

# Etapa 4: Montar o Pipeline
# O pipeline define a sequ√™ncia de opera√ß√µes do nosso fluxo de trabalho.
pipeline = Pipeline(stages=[label_indexer, vector_assembler, dt])
```

### 5. Treinamento e Avalia√ß√£o

Com o pipeline definido, o pr√≥ximo passo √© dividir nossos dados em um conjunto de treinamento e um de teste. Treinaremos o modelo nos dados de treinamento e avaliaremos sua performance nos dados de teste, que ele nunca viu antes.

```python
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Dividir os dados: 80% para treinamento, 20% para teste
(training_data, test_data) = payments_df.randomSplit([0.8, 0.2], seed=42)

print(f"Registros de Treinamento: {training_data.count()}")
print(f"Registros de Teste: {test_data.count()}")

# Treinar o pipeline
# O m√©todo fit() executa todos os est√°gios do pipeline nos dados de treinamento.
print("\nIniciando o treinamento do modelo...")
pipeline_model = pipeline.fit(training_data)
print("Treinamento conclu√≠do!")

# Fazer previs√µes nos dados de teste
# O m√©todo transform() aplica o pipeline treinado para gerar previs√µes.
predictions_df = pipeline_model.transform(test_data)

print("\nExibindo as previs√µes:")
# Note as novas colunas: label, features, rawPrediction, probability, e prediction
predictions_df.select("payment_type", "label", "prediction", "probability").show(10)

# Avaliar o modelo
# Usaremos a m√©trica "accuracy" (acur√°cia) para avaliar.
# A acur√°cia mede a porcentagem de previs√µes corretas.
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")

accuracy = evaluator.evaluate(predictions_df)
print(f"\nAcur√°cia do modelo no conjunto de teste: {accuracy:.2%}")
```

### 6. Conclus√£o

Parab√©ns! Voc√™ construiu, treinou e avaliou seu primeiro modelo de machine learning com PySpark MLlib. Embora a acur√°cia possa n√£o ser perfeita, o objetivo deste laborat√≥rio foi entender o processo e o paradigma da API `pyspark.ml`.

Voc√™ aprendeu a:

*   Preparar dados para machine learning usando `StringIndexer` e `VectorAssembler`.
*   Definir um `Estimator` (o modelo) e encadear tudo em um `Pipeline`.
*   Treinar o pipeline com `fit()` e fazer previs√µes com `transform()`.
*   Avaliar a performance do modelo usando um `Evaluator`.

Este fluxo de trabalho de Pipeline √© a base para a constru√ß√£o de modelos de machine learning muito mais complexos e robustos no Spark. Nos pr√≥ximos laborat√≥rios, poderemos explorar outros algoritmos, t√©cnicas de engenharia de features mais avan√ßadas e m√©todos para otimizar a performance do modelo, como cross-validation e hyperparameter tuning.
# M√≥dulo 5: Processamento de Dados em Tempo Real com Structured Streaming

## Cap√≠tulo 1: Introdu√ß√£o ao Structured Streaming

### 1.1. O que √© Structured Streaming?

**Structured Streaming** √© o motor de processamento de fluxos de dados (streaming) do Apache Spark, constru√≠do sobre a API de DataFrames e o motor Spark SQL. Ele foi projetado para tornar o desenvolvimento de aplica√ß√µes de streaming mais simples, robusto e tolerante a falhas.

A ideia central e revolucion√°ria do Structured Streaming √© tratar um fluxo de dados em tempo real como uma **tabela que √© continuamente atualizada**. Cada novo dado que chega ao fluxo √© como uma nova linha sendo anexada a essa tabela infinita.

Isso permite que voc√™ aplique as mesmas opera√ß√µes que j√° conhece da API de DataFrames (como `select`, `filter`, `groupBy`, `join`) em um fluxo de dados. O Spark se encarrega de executar essa consulta de forma incremental e cont√≠nua, atualizando o resultado √† medida que novos dados chegam.

> "Structured Streaming √© um motor de processamento de fluxo escal√°vel e tolerante a falhas, constru√≠do sobre o motor Spark SQL. Voc√™ pode expressar sua computa√ß√£o de streaming da mesma forma que expressaria uma computa√ß√£o em lote em dados est√°ticos." - Documenta√ß√£o Oficial do Apache Spark

### 1.2. O Modelo de Programa√ß√£o

O modelo de programa√ß√£o do Structured Streaming √© surpreendentemente simples:

1.  **Definir uma Fonte de Entrada (Input Source)**: Voc√™ come√ßa criando um DataFrame de streaming que representa o fluxo de dados de entrada. O Spark possui conectores para diversas fontes, como Apache Kafka, sistemas de arquivos (lendo novos arquivos em um diret√≥rio), sockets de rede, etc.

    ```python
    # Exemplo: Lendo um fluxo de arquivos CSV de um diret√≥rio
    input_df = spark.readStream \
        .schema(some_schema) \
        .csv("/path/to/input/dir")
    ```

2.  **Definir a Consulta (Query)**: Voc√™ aplica uma s√©rie de transforma√ß√µes (a "consulta") a este DataFrame de entrada, exatamente como faria com um DataFrame est√°tico.

    ```python
    # Exemplo: Contando eventos por tipo
    result_df = input_df \
        .groupBy("event_type") \
        .count()
    ```

3.  **Definir um Coletor de Sa√≠da (Output Sink)**: Voc√™ especifica para onde o resultado da sua consulta deve ser enviado. Isso pode ser a mem√≥ria (para depura√ß√£o), o console, um sistema de arquivos (escrevendo em formatos como Parquet ou CSV) ou o Kafka.

4.  **Iniciar a Consulta (Start the Query)**: Voc√™ inicia o processamento do fluxo. O Spark come√ßar√° a monitorar a fonte de entrada, processar os novos dados e atualizar o coletor de sa√≠da.

    ```python
    query = result_df.writeStream \
        .outputMode("complete") \
        .format("console") \
        .start()
    
    query.awaitTermination()
    ```

### 1.3. Modos de Sa√≠da (Output Modes)

O modo de sa√≠da define o que √© escrito no coletor de sa√≠da a cada vez que o resultado √© atualizado. Existem tr√™s modos principais:

*   **`append` (Anexar)**: (Padr√£o) Apenas as **novas linhas** adicionadas √† tabela de resultados desde o √∫ltimo gatilho ser√£o enviadas para o coletor. Isso √© √∫til para consultas onde voc√™ n√£o est√° agregando dados (ex: apenas filtrando).

*   **`complete` (Completo)**: A **tabela de resultados completa** ser√° enviada para o coletor a cada gatilho. Isso √© usado para consultas de agrega√ß√£o, onde voc√™ quer ver o resultado agregado atualizado a cada vez.

*   **`update` (Atualizar)**: Apenas as **linhas que foram atualizadas** na tabela de resultados desde o √∫ltimo gatilho ser√£o enviadas para o coletor. √â um meio-termo entre `append` e `complete`.

### 1.4. Toler√¢ncia a Falhas

Uma das caracter√≠sticas mais importantes do Structured Streaming √© sua garantia de processamento **exatamente uma vez (exactly-once)**. Isso significa que, mesmo que ocorram falhas nos n√≥s do cluster, o Spark garante que cada registro do fluxo de entrada ser√° processado exatamente uma vez, sem perda ou duplica√ß√£o de dados.

Isso √© alcan√ßado atrav√©s de duas t√©cnicas:

1.  **Checkpoints**: O Spark periodicamente salva o estado da sua consulta (quais dados j√° foram processados, os resultados intermedi√°rios das agrega√ß√µes, etc.) em um armazenamento confi√°vel (como HDFS ou S3). Se a aplica√ß√£o falhar, ela pode ser reiniciada a partir do √∫ltimo checkpoint, continuando de onde parou.
2.  **Write-Ahead Logs (WALs)**: Antes de processar um lote de dados, o Spark escreve as informa√ß√µes sobre esse lote em um log. Somente ap√≥s o processamento ser conclu√≠do com sucesso √© que o log √© atualizado. Isso evita que os dados sejam reprocessados em caso de falha.

### 1.5. Casos de Uso

O Structured Streaming √© ideal para uma ampla gama de aplica√ß√µes em tempo real:

*   **Monitoramento e Alertas**: An√°lise de logs de servidor ou m√©tricas de aplica√ß√£o em tempo real para detectar anomalias e enviar alertas.
*   **An√°lise de Dados de IoT**: Processamento de dados de sensores de dispositivos IoT para monitoramento de condi√ß√µes, manuten√ß√£o preditiva, etc.
*   **Detec√ß√£o de Fraudes**: An√°lise de transa√ß√µes financeiras ou de cliques em tempo real para identificar padr√µes fraudulentos.
*   **ETL em Tempo Real**: Limpeza, transforma√ß√£o e enriquecimento de dados √† medida que chegam, antes de carreg√°-los em um data warehouse ou data lake.
*   **Personaliza√ß√£o de Experi√™ncia do Usu√°rio**: An√°lise do comportamento do usu√°rio em um site ou aplicativo em tempo real para fornecer recomenda√ß√µes ou conte√∫do personalizado.

No pr√≥ximo laborat√≥rio, vamos colocar o Structured Streaming em pr√°tica, criando uma aplica√ß√£o simples para simular e processar um fluxo de dados de vendas em tempo real.
# M√≥dulo 5: Processamento de Dados em Tempo Real com Structured Streaming

## Laborat√≥rio: Simulando e Processando um Fluxo de Vendas em Tempo Real

### 1. Objetivo

Neste laborat√≥rio, vamos construir nossa primeira aplica√ß√£o de streaming com o Structured Streaming. Como nem sempre temos um fluxo de dados real (como o Kafka) dispon√≠vel em um ambiente de desenvolvimento, vamos **simular** um fluxo de dados usando o pr√≥prio Spark.

O cen√°rio ser√° o seguinte: vamos gerar arquivos CSV de vendas em um diret√≥rio a cada poucos segundos. Nossa aplica√ß√£o de Structured Streaming ir√° monitorar esse diret√≥rio, processar os novos arquivos assim que eles aparecerem e calcular uma agrega√ß√£o em tempo real: o **faturamento total por categoria de produto**.

Este laborat√≥rio ir√° ensin√°-lo a:

*   Usar uma fonte de streaming baseada em sistema de arquivos.
*   Aplicar transforma√ß√µes (`join`, `groupBy`) em um DataFrame de streaming.
*   Usar o modo de sa√≠da `complete` para exibir agrega√ß√µes atualizadas.
*   Escrever o resultado da consulta de streaming no console.

### 2. Preparando o Ambiente e os Dados

Primeiro, precisamos de um diret√≥rio para simular nosso fluxo de entrada e dos dados base para a jun√ß√£o.

```python
import os
import shutil

# Definir os caminhos para os diret√≥rios de streaming
input_path = "/tmp/streaming_input_vendas"
checkpoint_path = "/tmp/streaming_checkpoint_vendas"

# Limpar os diret√≥rios de execu√ß√µes anteriores para garantir um come√ßo limpo
if os.path.exists(input_path):
    shutil.rmtree(input_path)
if os.path.exists(checkpoint_path):
    shutil.rmtree(checkpoint_path)

# Criar o diret√≥rio de entrada
os.makedirs(input_path)

print(f"Diret√≥rio de entrada para o streaming criado em: {input_path}")
print(f"Diret√≥rio de checkpoint ser√° criado em: {checkpoint_path}")

# Carregar o DataFrame de produtos, que usaremos para o join
# Este √© um DataFrame est√°tico
produtos_df = spark.read.csv("produtos.csv", header=True, inferSchema=True)
```

### 3. Definindo a Aplica√ß√£o de Streaming

Agora, vamos construir nossa aplica√ß√£o passo a passo, conforme o modelo de programa√ß√£o do Structured Streaming.

**Passo 1: Definir a Fonte de Entrada**

Vamos configurar o Spark para ler arquivos CSV do diret√≥rio `input_path` como um fluxo. Precisamos fornecer o schema dos dados, pois a infer√™ncia de schema n√£o √© suportada para fontes de streaming baseadas em arquivo.

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType

# Definir o schema para os dados de vendas que vamos ler
vendas_schema = StructType([
    StructField("venda_id", IntegerType(), True),
    StructField("data_venda", TimestampType(), True),
    StructField("cliente_id", IntegerType(), True),
    StructField("produto_id", IntegerType(), True),
    StructField("quantidade", IntegerType(), True),
    StructField("preco_total", DoubleType(), True)
])

# Criar o DataFrame de streaming
streaming_vendas_df = spark.readStream \
    .schema(vendas_schema) \
    .option("maxFilesPerTrigger", 1)  # Processar um arquivo por vez
    .csv(input_path)
```

**Passo 2: Definir a Consulta**

Nossa consulta ir√° juntar o fluxo de vendas com os dados est√°ticos de produtos e depois calcular a soma do `preco_total` por `categoria`.

```python
# Juntar o DataFrame de streaming com o DataFrame est√°tico de produtos
streaming_vendas_com_categoria_df = streaming_vendas_df.join(
    produtos_df, 
    streaming_vendas_df.produto_id == produtos_df.produto_id, 
    "inner"
)

# Agrupar por categoria e calcular o faturamento total
faturamento_por_categoria_df = streaming_vendas_com_categoria_df.groupBy("categoria") \
    .sum("preco_total") \
    .withColumnRenamed("sum(preco_total)", "faturamento_total") \
    .orderBy("faturamento_total", ascending=False)
```

**Passo 3: Definir o Coletor de Sa√≠da e Iniciar a Consulta**

Vamos escrever o resultado da nossa agrega√ß√£o no console. Como √© uma agrega√ß√£o, usaremos o modo de sa√≠da `complete` para ver a tabela de resultados inteira a cada atualiza√ß√£o.

```python
# Escrever o resultado no console
query = faturamento_por_categoria_df.writeStream \
    .outputMode("complete") \
    .format("console") \
    .option("truncate", False) \
    .option("checkpointLocation", checkpoint_path) \
    .start()

print("Aplica√ß√£o de streaming iniciada! Monitorando o diret√≥rio de entrada...")
```

### 4. Simulando o Fluxo de Dados

Com a nossa consulta de streaming rodando, ela est√° agora esperando por novos arquivos no diret√≥rio `/tmp/streaming_input_vendas`. Vamos criar uma fun√ß√£o para gerar pequenos arquivos CSV de vendas e salv√°-los nesse diret√≥rio para simular um fluxo.

**Execute o c√≥digo abaixo em uma c√©lula separada** para que voc√™ possa execut√°-lo v√°rias vezes e ver a sa√≠da da sua consulta de streaming sendo atualizada.

```python
import time

# Pegar algumas linhas do DataFrame de vendas original para usar como dados de amostra
vendas_df = spark.read.csv("vendas.csv", header=True, inferSchema=True)
sample_data = vendas_df.take(20) # Pegar as primeiras 20 vendas

# Fun√ß√£o para gerar um novo arquivo de vendas
def gerar_novo_arquivo_vendas(file_index):
    # Selecionar um pequeno lote de dados de amostra
    start = (file_index * 5) % len(sample_data)
    end = start + 5
    batch_df = spark.createDataFrame(sample_data[start:end])
    
    # Nome do arquivo
    file_name = f"vendas_{int(time.time())}.csv"
    output_file_path = os.path.join(input_path, file_name)
    
    # Escrever o lote como um √∫nico arquivo CSV no diret√≥rio de entrada
    # O Spark escreve em um diret√≥rio, ent√£o precisamos mover o arquivo gerado
    temp_path = f"/tmp/temp_vendas_{file_index}"
    batch_df.coalesce(1).write.csv(temp_path, header=True, mode="overwrite")
    
    # Encontrar o arquivo CSV gerado e mov√™-lo para o diret√≥rio de entrada
    part_file = [f for f in os.listdir(os.path.join(temp_path)) if f.endswith(".csv")][0]
    shutil.move(os.path.join(temp_path, part_file), output_file_path)
    shutil.rmtree(temp_path)
    
    print(f"‚úîÔ∏è Novo arquivo de vendas gerado: {output_file_path}")

# Gerar alguns arquivos para iniciar
for i in range(3):
    gerar_novo_arquivo_vendas(i)
    time.sleep(5) # Esperar um pouco entre os arquivos
```

### 5. Observando os Resultados

Volte para a c√©lula onde a consulta de streaming est√° rodando. Voc√™ ver√° que, a cada novo arquivo CSV que √© salvo no diret√≥rio de entrada, o Spark o processa e atualiza a tabela de faturamento por categoria no console. A sa√≠da ser√° algo como:

```
-------------------------------------------
Batch: 0
-------------------------------------------
+---------------+------------------+
|categoria      |faturamento_total |
+---------------+------------------+
|Eletr√¥nicos    |2848.5            |
|Brinquedos     |42.33             |
|Esportes       |765.15            |
+---------------+------------------+

-------------------------------------------
Batch: 1
-------------------------------------------
+---------------+------------------+
|categoria      |faturamento_total |
+---------------+------------------+
|Casa e Cozinha |3461.05           |
|Roupas         |2852.15           |
|Eletr√¥nicos    |2848.5            |
|Ferramentas    |535.83            |
|Esportes       |765.15            |
|Brinquedos     |42.33             |
+---------------+------------------+
```

### 6. Parando a Consulta

Para parar a execu√ß√£o da consulta de streaming, voc√™ pode usar o m√©todo `stop()`.

```python
# Para parar a consulta
query.stop()
print("Aplica√ß√£o de streaming parada.")
```

### 7. Conclus√£o

Neste laborat√≥rio, voc√™ construiu sua primeira aplica√ß√£o de ponta a ponta com o Structured Streaming. Voc√™ aprendeu a ler um fluxo de dados de um sistema de arquivos, aplicar transforma√ß√µes complexas como joins e agrega√ß√µes, e exibir o resultado em tempo real no console.

O mais importante √© que voc√™ viu como a API do Structured Streaming permite reutilizar todo o seu conhecimento sobre a API de DataFrames para processar dados em tempo real, tornando o desenvolvimento de aplica√ß√µes de streaming muito mais simples e intuitivo.
# M√≥dulo 6: Otimiza√ß√£o e Performance no Spark

## Cap√≠tulo 1: Conceitos Fundamentais de Otimiza√ß√£o

### 1.1. Por que a Otimiza√ß√£o √© Importante?

Escrever c√≥digo Spark que funciona √© apenas o primeiro passo. Em um ambiente de Big Data, escrever c√≥digo que funciona de forma **eficiente** √© crucial. Uma consulta mal otimizada pode levar horas para ser executada, consumir recursos excessivos do cluster e aumentar os custos operacionais. Por outro lado, uma consulta bem otimizada pode ser executada em minutos, usando os recursos de forma eficiente.

O Spark j√° faz um trabalho fant√°stico de otimiza√ß√£o autom√°tica atrav√©s do Catalyst Optimizer, mas h√° v√°rias t√©cnicas e configura√ß√µes que voc√™, como desenvolvedor, pode aplicar para extrair o m√°ximo de performance da sua aplica√ß√£o.

Neste m√≥dulo, vamos explorar os conceitos mais importantes de otimiza√ß√£o no Spark, focando em particionamento, cache e a an√°lise da Spark UI.

### 1.2. Particionamento: A Base da Paraleliza√ß√£o

O **particionamento** √© o conceito mais fundamental para entender a performance no Spark. Um DataFrame n√£o √© uma √∫nica unidade de dados; ele √© dividido em v√°rias partes menores chamadas **parti√ß√µes**. Cada parti√ß√£o √© uma cole√ß√£o de linhas que reside em um n√≥ executor no cluster.

O n√∫mero de parti√ß√µes determina o **grau de paralelismo** da sua aplica√ß√£o. Se o seu DataFrame tem 100 parti√ß√µes, o Spark pode, teoricamente, executar 100 tarefas em paralelo para processar esses dados (assumindo que o cluster tenha recursos suficientes).

**Como o Spark decide o n√∫mero de parti√ß√µes?**

*   **Na Leitura**: Ao ler dados de um sistema de arquivos como HDFS ou S3, o Spark geralmente cria uma parti√ß√£o para cada bloco do arquivo (por exemplo, um bloco de 128 MB).
*   **Ap√≥s um Shuffle**: Opera√ß√µes como `groupBy`, `join`, e `repartition` causam um **shuffle**, que redistribui os dados. Por padr√£o, o n√∫mero de parti√ß√µes ap√≥s um shuffle √© controlado pela configura√ß√£o `spark.sql.shuffle.partitions` (o padr√£o √© 200).

**O Dilema do Particionamento:**

*   **Muitas Parti√ß√µes Pequenas**: Causa uma sobrecarga no Driver, que precisa gerenciar um grande n√∫mero de tarefas. Cada tarefa tem um pequeno custo de inicializa√ß√£o, e a soma desses custos pode se tornar significativa.
*   **Poucas Parti√ß√µes Grandes**: Leva a um baixo paralelismo. Se voc√™ tem apenas 4 parti√ß√µes, no m√°ximo 4 tarefas podem ser executadas ao mesmo tempo, deixando o resto do cluster ocioso. Al√©m disso, parti√ß√µes muito grandes podem causar problemas de mem√≥ria nos executores.

O objetivo √© encontrar um equil√≠brio: ter parti√ß√µes suficientes para maximizar o paralelismo, mas n√£o tantas a ponto de sobrecarregar o sistema. Uma boa regra geral √© ter de 2 a 4 parti√ß√µes por core de CPU no seu cluster.

### 1.3. O Shuffle: O Grande Vil√£o da Performance

O **shuffle** √© o processo de redistribuir os dados entre as parti√ß√µes. Ele √© necess√°rio para opera√ß√µes que precisam ver todos os valores de uma mesma chave para produzir um resultado, como `groupBy` (para agrupar todas as linhas com a mesma chave) e `join` (para juntar linhas com a mesma chave de jun√ß√£o).

O shuffle √© uma opera√ß√£o extremamente custosa por duas raz√µes:

1.  **I/O de Rede**: Envolve a transfer√™ncia de grandes volumes de dados pela rede entre os executores.
2.  **I/O de Disco**: Os dados s√£o primeiro escritos em disco pelos executores de origem (na fase de *map*) e depois lidos do disco pelos executadores de destino (na fase de *reduce*).

**Minimizar o n√∫mero de shuffles √© a otimiza√ß√£o mais importante que voc√™ pode fazer em uma aplica√ß√£o Spark.**

### 1.4. Cache e Persist√™ncia: Evitando Rec√°lculos

Por padr√£o, o Spark reavalia um DataFrame e todas as suas transforma√ß√µes a partir da fonte original toda vez que uma a√ß√£o √© chamada. Se voc√™ usa o mesmo DataFrame em m√∫ltiplas a√ß√µes, isso pode ser muito ineficiente.

Para evitar esse rec√°lculo, voc√™ pode **persistir** um DataFrame em mem√≥ria (e/ou em disco). Quando voc√™ persiste um DataFrame, o Spark armazena suas parti√ß√µes na primeira vez que ele √© calculado. Nas vezes seguintes que voc√™ usar esse DataFrame, o Spark simplesmente ler√° as parti√ß√µes da mem√≥ria, o que √© ordens de magnitude mais r√°pido do que recalcular tudo.

Os dois principais m√©todos para isso s√£o:

*   **`cache()`**: √â um atalho para `persist(StorageLevel.MEMORY_ONLY)`. Ele armazena as parti√ß√µes do DataFrame na mem√≥ria da JVM dos executores.
*   **`persist(storage_level)`**: Oferece mais controle sobre como os dados s√£o armazenados. Voc√™ pode escolher diferentes n√≠veis de armazenamento, como:
    *   `MEMORY_ONLY`: Apenas mem√≥ria (padr√£o do `cache()`).
    *   `MEMORY_AND_DISK`: Armazena na mem√≥ria. Se n√£o houver espa√ßo suficiente, as parti√ß√µes que n√£o couberem s√£o armazenadas em disco.
    *   `DISK_ONLY`: Armazena apenas em disco.

**Quando usar `cache()`?**

Use `cache()` (ou `persist()`) em DataFrames que s√£o usados m√∫ltiplas vezes em sua aplica√ß√£o, especialmente se eles s√£o o resultado de transforma√ß√µes caras (como joins ou agrega√ß√µes).

```python
# resultado_intermediario_df √© o resultado de um join caro
resultado_intermediario_df = df1.join(df2, "id")

# Persistimos o resultado em mem√≥ria
resultado_intermediario_df.cache()

# Primeira a√ß√£o: dispara o c√°lculo e o cache
print(f"Contagem: {resultado_intermediario_df.count()}")

# Segunda a√ß√£o: reutiliza os dados cacheados, muito mais r√°pido
resultado_intermediario_df.filter("coluna > 10").show()
```

### 1.5. A Spark UI: Sua Ferramenta de Diagn√≥stico

A **Spark UI** √© uma interface web que fornece uma vis√£o detalhada sobre sua aplica√ß√£o Spark. √â a ferramenta mais importante para entender o que est√° acontecendo por baixo dos panos, diagnosticar problemas de performance e validar suas otimiza√ß√µes.

Na Spark UI, voc√™ pode ver:

*   **Jobs**: As a√ß√µes que dispararam a execu√ß√£o.
*   **Stages**: Os est√°gios em que cada job foi dividido (separados por shuffles).
*   **Tasks**: As tarefas individuais executadas para cada est√°gio.
*   **O plano de execu√ß√£o SQL**: O plano l√≥gico e f√≠sico gerado pelo Catalyst Optimizer.
*   **Informa√ß√µes sobre armazenamento**: Quais DataFrames est√£o em cache e quanto espa√ßo est√£o usando.

No pr√≥ximo laborat√≥rio, vamos aplicar esses conceitos na pr√°tica e usar a Spark UI para observar o impacto de nossas otimiza√ß√µes.
# M√≥dulo 6: Otimiza√ß√£o e Performance no Spark

## Laborat√≥rio: Otimizando uma An√°lise de Vendas

### 1. Objetivo

Neste laborat√≥rio, vamos colocar em pr√°tica os conceitos de otimiza√ß√£o que aprendemos. Vamos escrever uma consulta que, intencionalmente, n√£o √© otimizada e, em seguida, aplicar t√©cnicas de **cache** e **reparticionamento** para melhorar drasticamente sua performance.

O mais importante √© que vamos aprender a usar a **Spark UI** para **diagnosticar** os gargalos de performance e **verificar** o impacto de nossas otimiza√ß√µes. No Google Colab, a Spark UI √© acess√≠vel atrav√©s de um t√∫nel `ngrok`.

O cen√°rio ser√° uma an√°lise que envolve m√∫ltiplas a√ß√µes sobre um DataFrame resultante de um join caro.

### 2. Configurando a Spark UI no Google Colab

Para acessar a Spark UI no Colab, precisamos expor a porta em que ela roda (porta 4040) para a internet. Faremos isso usando o `ngrok`.

```python
# Instalar o pyngrok
!pip -q install pyngrok

from pyngrok import ngrok

# Abrir um t√∫nel p√∫blico para a porta 4040 (porta padr√£o da Spark UI)
# Substitua <SEU_AUTHTOKEN> pelo seu token do ngrok (obtenha em https://dashboard.ngrok.com/get-started/your-authtoken)
# Se n√£o tiver um token, pode funcionar, mas com limita√ß√µes.
ngrok.set_auth_token("<SEU_AUTHTOKEN>")
public_url = ngrok.connect(4040)

print(f"Spark UI rodando em: {public_url}")
```

**Como usar:** Ap√≥s iniciar sua `SparkSession`, clique no link gerado pelo `ngrok`. Isso abrir√° a Spark UI em uma nova aba. Mantenha essa aba aberta para acompanhar a execu√ß√£o das suas consultas.

### 3. Cen√°rio de An√°lise: Vers√£o N√£o Otimizada

Vamos realizar uma an√°lise que envolve juntar os DataFrames de vendas e produtos e, em seguida, executar tr√™s a√ß√µes diferentes sobre o resultado.

```python
from pyspark.sql import SparkSession

# Iniciar a SparkSession
spark = SparkSession.builder.master("local[*]").appName("CursoHadoop_LabOtimizacao").getOrCreate()

# Carregar os dados
vendas_df = spark.read.csv("vendas.csv", header=True, inferSchema=True)
produtos_df = spark.read.csv("produtos.csv", header=True, inferSchema=True)

# Juntar os dois DataFrames. Esta √© uma opera√ß√£o cara que envolve um shuffle.
vendas_produtos_df = vendas_df.join(produtos_df, "produto_id", "inner")

# --- VERS√ÉO N√ÉO OTIMIZADA ---

# A√ß√£o 1: Contar o n√∫mero total de registros
print("Executando A√ß√£o 1...")
count = vendas_produtos_df.count()
print(f"Total de registros: {count}")

# A√ß√£o 2: Calcular o faturamento total por categoria
print("\nExecutando A√ß√£o 2...")
vendas_produtos_df.groupBy("categoria").sum("preco_total").show()

# A√ß√£o 3: Encontrar os 5 produtos com maior quantidade vendida
print("\nExecutando A√ß√£o 3...")
vendas_produtos_df.groupBy("nome_produto").sum("quantidade").orderBy("sum(quantidade)", ascending=False).show(5)
```

**An√°lise na Spark UI (Vers√£o N√£o Otimizada):**

1.  V√° para a aba "Jobs" na Spark UI. Voc√™ ver√° **tr√™s jobs separados**, um para cada a√ß√£o (`count`, `show`, `show`).
2.  Clique em um dos jobs e depois no est√°gio que menciona o `join`. Voc√™ ver√° que o join e o shuffle associado a ele foram **executados tr√™s vezes**, uma para cada job.
3.  Isso √© extremamente ineficiente. Estamos refazendo a opera√ß√£o de join, que √© a mais cara, a cada a√ß√£o.

### 4. Otimizando com `cache()`

Agora, vamos aplicar a otimiza√ß√£o mais simples e eficaz para este cen√°rio: `cache()`. Vamos persistir o resultado do join em mem√≥ria ap√≥s a primeira vez que ele for calculado.

```python
# Juntar os dois DataFrames
vendas_produtos_df = vendas_df.join(produtos_df, "produto_id", "inner")

# --- VERS√ÉO OTIMIZADA COM CACHE ---

# Colocar o DataFrame resultante do join em cache
vendas_produtos_df.cache()

# A√ß√£o 1: Contar o n√∫mero total de registros
# Esta a√ß√£o ir√° disparar o c√°lculo do join e o armazenamento em cache.
print("Executando A√ß√£o 1 (com cache)...")
count = vendas_produtos_df.count()
print(f"Total de registros: {count}")

# A√ß√£o 2: Calcular o faturamento total por categoria
# Esta a√ß√£o ir√° ler o DataFrame diretamente da mem√≥ria, sem refazer o join.
print("\nExecutando A√ß√£o 2 (com cache)...")
vendas_produtos_df.groupBy("categoria").sum("preco_total").show()

# A√ß√£o 3: Encontrar os 5 produtos com maior quantidade vendida
# Esta a√ß√£o tamb√©m reutilizar√° o cache.
print("\nExecutando A√ß√£o 3 (com cache)...")
vendas_produtos_df.groupBy("nome_produto").sum("quantidade").orderBy("sum(quantidade)", ascending=False).show(5)

# Limpar o cache quando n√£o for mais necess√°rio
vendas_produtos_df.unpersist()
```

**An√°lise na Spark UI (Vers√£o com Cache):**

1.  V√° para a aba "Jobs". Voc√™ ainda ver√° tr√™s jobs.
2.  Clique no **primeiro job** (`count`). Voc√™ ver√° que ele executa o join normalmente.
3.  Agora, clique no **segundo e terceiro jobs**. Voc√™ ver√° que os est√°gios que antes faziam o join agora s√£o muito mais r√°pidos e t√™m uma nova etapa chamada "RDD Scan" ou "InMemoryTableScan". Isso indica que eles est√£o lendo os dados diretamente do cache, pulando o join e o shuffle.
4.  V√° para a aba "Storage". Voc√™ ver√° seu DataFrame `vendas_produtos_df` listado, com informa√ß√µes sobre quanto da mem√≥ria ele est√° ocupando.

O tempo de execu√ß√£o do segundo e terceiro jobs ser√° significativamente menor.

### 5. Otimizando o Particionamento com `repartition()`

Vamos supor que, ap√≥s o join, nosso DataFrame `vendas_produtos_df` tenha muitas parti√ß√µes pequenas ou poucas parti√ß√µes grandes. Isso pode levar a um processamento ineficiente nas etapas seguintes. Podemos usar `repartition()` para otimizar o n√∫mero de parti√ß√µes.

**Quando usar `repartition()`?**

*   Use `repartition()` para **aumentar ou diminuir** o n√∫mero de parti√ß√µes. Ele sempre causa um **shuffle completo**, o que √© caro, ent√£o use-o com modera√ß√£o.
*   √â √∫til ap√≥s um filtro que reduz drasticamente o tamanho dos dados, deixando muitas parti√ß√µes vazias ou pequenas.
*   √â √∫til antes de uma opera√ß√£o que se beneficia de um particionamento espec√≠fico (que veremos em otimiza√ß√µes avan√ßadas).

```python
# Verificar o n√∫mero de parti√ß√µes ap√≥s o join
num_particoes_inicial = vendas_produtos_df.rdd.getNumPartitions()
print(f"N√∫mero de parti√ß√µes ap√≥s o join: {num_particoes_inicial}")

# Vamos supor que queremos trabalhar com 8 parti√ß√µes para as pr√≥ximas etapas
# Em um cluster real, este n√∫mero seria baseado no n√∫mero de cores dispon√≠veis
vendas_produtos_repart_df = vendas_produtos_df.repartition(8)

# Colocar em cache a vers√£o reparticionada
vendas_produtos_repart_df.cache()

# Executar as a√ß√µes novamente no DataFrame reparticionado
print("\nExecutando a√ß√µes no DataFrame reparticionado...")
vendas_produtos_repart_df.count()
vendas_produtos_repart_df.groupBy("categoria").sum("preco_total").show()

vendas_produtos_repart_df.unpersist()
```

**An√°lise na Spark UI (com `repartition`):**

1.  Na aba "SQL / DataFrame", encontre a consulta que executou o `repartition`.
2.  Voc√™ ver√° um est√°gio de "Exchange" que corresponde ao shuffle causado pelo `repartition`.
3.  Os jobs subsequentes que usam o DataFrame `vendas_produtos_repart_df` agora ser√£o executados com exatamente 8 tarefas em paralelo (uma para cada parti√ß√£o).

### 6. Conclus√£o

Neste laborat√≥rio, voc√™ aprendeu a usar a Spark UI como uma ferramenta de diagn√≥stico e aplicou duas das t√©cnicas de otimiza√ß√£o mais importantes:

*   **`cache()`**: Para evitar o rec√°lculo de transforma√ß√µes caras quando um DataFrame √© usado v√°rias vezes.
*   **`repartition()`**: Para controlar o grau de paralelismo da sua aplica√ß√£o, ajustando o n√∫mero de parti√ß√µes.

Dominar a otimiza√ß√£o √© um processo cont√≠nuo de **medir, analisar e refatorar**. A Spark UI √© sua melhor amiga nesse processo. Sempre que estiver lidando com grandes volumes de dados, use-a para entender como suas consultas est√£o sendo executadas e onde est√£o os gargalos.
